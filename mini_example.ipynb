{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qutip as qt\n",
    "from qutip_qip.operations.gates import hadamard_transform as hadamard\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from diffrax import Dopri5, Dopri8, Tsit5, PIDController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimize import optimize_pulses\n",
    "from time_interval import TimeInterval\n",
    "from objective import Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = qt.qeye(2)\n",
    "target  = hadamard()\n",
    "\n",
    "initial = qt.sprepost(initial, initial.dag())\n",
    "target  = qt.sprepost(target , target.dag() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "σx = qt.sigmax()\n",
    "σy = qt.sigmay()\n",
    "σz = qt.sigmaz()\n",
    "\n",
    "ω, Δ, γ, π = 0.1, 1.0, 0.1, np.pi\n",
    "\n",
    "Hd = 1/2 * (ω * σz + Δ * σx)\n",
    "\n",
    "H_d =  qt.liouvillian(H=Hd, c_ops=[np.sqrt(γ) * qt.sigmam()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = TimeInterval(tlist=[0, 2*π])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin(t, α):\n",
    "    return α[0] * np.sin(α[1] * t + α[2])\n",
    "\n",
    "def grad_sin(t, α, idx):\n",
    "    if idx==0: return np.sin(α[1] * t + α[2])\n",
    "    if idx==1: return α[0] * np.cos(α[1] * t + α[2]) * t\n",
    "    if idx==2: return α[0] * np.cos(α[1] * t + α[2])\n",
    "    if idx==3: return α[0] * np.cos(α[1] * t + α[2]) * α[1] # w.r.t. time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_x = lambda t, p: sin(t, p)\n",
    "sin_y = lambda t, q: sin(t, q)\n",
    "sin_z = lambda t, r: sin(t, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hc  = [σx, σy, σz]\n",
    "H_c = [qt.liouvillian(H) for H in Hc]\n",
    "\n",
    "H = [ H_d,\n",
    "     [H_c[0], sin_x, {\"grad\": grad_sin}],\n",
    "     [H_c[1], sin_y, {\"grad\": grad_sin}],\n",
    "     [H_c[2], sin_z, {\"grad\": grad_sin}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_init = [1, 1, 0]\n",
    "q_init = [1, 1, 0]\n",
    "r_init = [1, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default integrator settings - local search only - fix time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GOAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimizer step, infidelity: 0.27608\n",
      "minimizer step, infidelity: 0.26606\n",
      "minimizer step, infidelity: 0.24657\n",
      "minimizer step, infidelity: 0.19541\n",
      "minimizer step, infidelity: 0.11592\n",
      "minimizer step, infidelity: 0.09042\n",
      "minimizer step, infidelity: 0.07068\n",
      "minimizer step, infidelity: 0.06814\n",
      "minimizer step, infidelity: 0.06529\n",
      "minimizer step, infidelity: 0.06451\n",
      "minimizer step, infidelity: 0.06267\n",
      "minimizer step, infidelity: 0.05990\n",
      "minimizer step, infidelity: 0.05549\n",
      "minimizer step, infidelity: 0.05311\n",
      "minimizer step, infidelity: 0.05289\n",
      "minimizer step, infidelity: 0.05285\n",
      "minimizer step, infidelity: 0.05276\n",
      "minimizer step, infidelity: 0.05264\n",
      "minimizer step, infidelity: 0.05248\n",
      "minimizer step, infidelity: 0.05234\n",
      "minimizer step, infidelity: 0.05186\n",
      "minimizer step, infidelity: 0.05125\n",
      "minimizer step, infidelity: 0.05072\n",
      "minimizer step, infidelity: 0.05029\n",
      "minimizer step, infidelity: 0.05026\n",
      "minimizer step, infidelity: 0.05022\n",
      "minimizer step, infidelity: 0.05017\n",
      "minimizer step, infidelity: 0.05011\n",
      "minimizer step, infidelity: 0.05006\n",
      "minimizer step, infidelity: 0.05001\n",
      "minimizer step, infidelity: 0.04999\n",
      "minimizer step, infidelity: 0.04997\n",
      "minimizer step, infidelity: 0.04997\n",
      "minimizer step, infidelity: 0.04995\n",
      "minimizer step, infidelity: 0.04994\n",
      "minimizer step, infidelity: 0.04993\n",
      "minimizer step, infidelity: 0.04990\n",
      "minimizer step, infidelity: 0.04987\n",
      "minimizer step, infidelity: 0.04986\n",
      "minimizer step, infidelity: 0.04980\n",
      "minimizer step, infidelity: 0.04967\n",
      "minimizer step, infidelity: 0.04961\n",
      "minimizer step, infidelity: 0.04960\n",
      "minimizer step, infidelity: 0.04960\n",
      "minimizer step, infidelity: 0.04958\n",
      "minimizer step, infidelity: 0.04952\n",
      "minimizer step, infidelity: 0.04949\n",
      "minimizer step, infidelity: 0.04949\n",
      "minimizer step, infidelity: 0.04934\n",
      "minimizer step, infidelity: 0.04933\n",
      "minimizer step, infidelity: 0.04920\n",
      "minimizer step, infidelity: 0.04911\n",
      "minimizer step, infidelity: 0.04896\n",
      "minimizer step, infidelity: 0.04879\n",
      "minimizer step, infidelity: 0.04838\n",
      "minimizer step, infidelity: 0.04825\n",
      "minimizer step, infidelity: 0.04797\n",
      "minimizer step, infidelity: 0.04788\n",
      "minimizer step, infidelity: 0.04781\n",
      "minimizer step, infidelity: 0.04777\n",
      "minimizer step, infidelity: 0.04771\n",
      "minimizer step, infidelity: 0.04768\n",
      "minimizer step, infidelity: 0.04757\n",
      "minimizer step, infidelity: 0.04751\n",
      "minimizer step, infidelity: 0.04743\n",
      "minimizer step, infidelity: 0.04729\n",
      "minimizer step, infidelity: 0.04696\n",
      "minimizer step, infidelity: 0.04649\n",
      "minimizer step, infidelity: 0.04618\n",
      "minimizer step, infidelity: 0.04596\n",
      "minimizer step, infidelity: 0.04593\n",
      "minimizer step, infidelity: 0.04578\n",
      "minimizer step, infidelity: 0.04576\n",
      "minimizer step, infidelity: 0.04571\n",
      "minimizer step, infidelity: 0.04570\n",
      "minimizer step, infidelity: 0.04570\n",
      "minimizer step, infidelity: 0.04570\n",
      "minimizer step, infidelity: 0.04570\n",
      "minimizer step, infidelity: 0.04570\n",
      "minimizer step, infidelity: 0.04570\n",
      "minimizer step, infidelity: 0.04570\n",
      "minimizer step, infidelity: 0.04570\n",
      "minimizer step, infidelity: 0.04569\n",
      "minimizer step, infidelity: 0.04568\n",
      "minimizer step, infidelity: 0.04565\n",
      "minimizer step, infidelity: 0.04563\n",
      "minimizer step, infidelity: 0.04563\n",
      "minimizer step, infidelity: 0.04562\n",
      "minimizer step, infidelity: 0.04562\n",
      "minimizer step, infidelity: 0.04561\n",
      "minimizer step, infidelity: 0.04561\n",
      "minimizer step, infidelity: 0.04560\n",
      "minimizer step, infidelity: 0.04559\n",
      "minimizer step, infidelity: 0.04559\n",
      "minimizer step, infidelity: 0.04559\n",
      "minimizer step, infidelity: 0.04559\n",
      "minimizer step, infidelity: 0.04559\n",
      "minimizer step, infidelity: 0.04558\n",
      "minimizer step, infidelity: 0.04558\n",
      "minimizer step, infidelity: 0.04558\n",
      "minimizer step, infidelity: 0.04558\n",
      "minimizer step, infidelity: 0.04558\n",
      "minimizer step, infidelity: 0.04558\n",
      "minimizer step, infidelity: 0.04558\n",
      "minimizer step, infidelity: 0.04558\n",
      "minimizer step, infidelity: 0.04557\n",
      "minimizer step, infidelity: 0.04556\n",
      "minimizer step, infidelity: 0.04554\n",
      "minimizer step, infidelity: 0.04553\n",
      "minimizer step, infidelity: 0.04550\n",
      "minimizer step, infidelity: 0.04549\n",
      "minimizer step, infidelity: 0.04546\n",
      "minimizer step, infidelity: 0.04543\n",
      "minimizer step, infidelity: 0.04539\n",
      "minimizer step, infidelity: 0.04535\n",
      "minimizer step, infidelity: 0.04530\n",
      "minimizer step, infidelity: 0.04525\n",
      "minimizer step, infidelity: 0.04523\n",
      "minimizer step, infidelity: 0.04522\n",
      "minimizer step, infidelity: 0.04520\n",
      "minimizer step, infidelity: 0.04520\n",
      "minimizer step, infidelity: 0.04519\n",
      "minimizer step, infidelity: 0.04518\n",
      "minimizer step, infidelity: 0.04516\n",
      "minimizer step, infidelity: 0.04512\n",
      "minimizer step, infidelity: 0.04506\n",
      "minimizer step, infidelity: 0.04502\n",
      "minimizer step, infidelity: 0.04499\n",
      "minimizer step, infidelity: 0.04496\n",
      "minimizer step, infidelity: 0.04494\n",
      "minimizer step, infidelity: 0.04492\n",
      "minimizer step, infidelity: 0.04488\n",
      "minimizer step, infidelity: 0.04486\n",
      "minimizer step, infidelity: 0.04484\n",
      "minimizer step, infidelity: 0.04481\n",
      "minimizer step, infidelity: 0.04479\n",
      "minimizer step, infidelity: 0.04476\n",
      "minimizer step, infidelity: 0.04473\n",
      "minimizer step, infidelity: 0.04472\n",
      "minimizer step, infidelity: 0.04472\n",
      "minimizer step, infidelity: 0.04470\n",
      "minimizer step, infidelity: 0.04470\n",
      "minimizer step, infidelity: 0.04470\n",
      "minimizer step, infidelity: 0.04470\n",
      "minimizer step, infidelity: 0.04469\n",
      "minimizer step, infidelity: 0.04468\n",
      "minimizer step, infidelity: 0.04468\n",
      "minimizer step, infidelity: 0.04466\n",
      "minimizer step, infidelity: 0.04465\n",
      "minimizer step, infidelity: 0.04465\n",
      "minimizer step, infidelity: 0.04465\n",
      "minimizer step, infidelity: 0.04464\n",
      "minimizer step, infidelity: 0.04464\n",
      "minimizer step, infidelity: 0.04463\n",
      "minimizer step, infidelity: 0.04463\n",
      "minimizer step, infidelity: 0.04463\n",
      "minimizer step, infidelity: 0.04463\n",
      "minimizer step, infidelity: 0.04463\n",
      "minimizer step, infidelity: 0.04463\n",
      "minimizer step, infidelity: 0.04463\n",
      "minimizer step, infidelity: 0.04463\n",
      "minimizer step, infidelity: 0.04463\n",
      "minimizer step, infidelity: 0.04462\n",
      "minimizer step, infidelity: 0.04462\n",
      "minimizer step, infidelity: 0.04462\n",
      "minimizer step, infidelity: 0.04461\n",
      "minimizer step, infidelity: 0.04460\n",
      "minimizer step, infidelity: 0.04460\n",
      "minimizer step, infidelity: 0.04459\n",
      "minimizer step, infidelity: 0.04458\n",
      "minimizer step, infidelity: 0.04457\n",
      "minimizer step, infidelity: 0.04455\n",
      "minimizer step, infidelity: 0.04453\n",
      "minimizer step, infidelity: 0.04451\n",
      "minimizer step, infidelity: 0.04449\n",
      "minimizer step, infidelity: 0.04447\n",
      "minimizer step, infidelity: 0.04444\n",
      "minimizer step, infidelity: 0.04443\n",
      "minimizer step, infidelity: 0.04441\n",
      "minimizer step, infidelity: 0.04439\n",
      "minimizer step, infidelity: 0.04436\n",
      "minimizer step, infidelity: 0.04435\n",
      "minimizer step, infidelity: 0.04434\n",
      "minimizer step, infidelity: 0.04433\n",
      "minimizer step, infidelity: 0.04431\n",
      "minimizer step, infidelity: 0.04429\n",
      "minimizer step, infidelity: 0.04426\n",
      "minimizer step, infidelity: 0.04425\n",
      "minimizer step, infidelity: 0.04423\n",
      "minimizer step, infidelity: 0.04423\n",
      "minimizer step, infidelity: 0.04422\n",
      "minimizer step, infidelity: 0.04421\n",
      "minimizer step, infidelity: 0.04421\n",
      "minimizer step, infidelity: 0.04420\n",
      "minimizer step, infidelity: 0.04420\n",
      "minimizer step, infidelity: 0.04420\n",
      "minimizer step, infidelity: 0.04420\n",
      "minimizer step, infidelity: 0.04420\n",
      "minimizer step, infidelity: 0.04419\n",
      "minimizer step, infidelity: 0.04419\n",
      "minimizer step, infidelity: 0.04419\n",
      "minimizer step, infidelity: 0.04418\n",
      "minimizer step, infidelity: 0.04418\n",
      "minimizer step, infidelity: 0.04417\n",
      "minimizer step, infidelity: 0.04415\n",
      "minimizer step, infidelity: 0.04414\n",
      "minimizer step, infidelity: 0.04414\n",
      "minimizer step, infidelity: 0.04414\n",
      "minimizer step, infidelity: 0.04413\n",
      "minimizer step, infidelity: 0.04413\n",
      "minimizer step, infidelity: 0.04413\n",
      "minimizer step, infidelity: 0.04413\n",
      "minimizer step, infidelity: 0.04413\n",
      "minimizer step, infidelity: 0.04413\n",
      "minimizer step, infidelity: 0.04413\n",
      "minimizer step, infidelity: 0.04413\n",
      "minimizer step, infidelity: 0.04413\n",
      "minimizer step, infidelity: 0.04413\n",
      "minimizer step, infidelity: 0.04413\n",
      "minimizer step, infidelity: 0.04413\n",
      "minimizer step, infidelity: 0.04413\n",
      "minimizer step, infidelity: 0.04413\n",
      "minimizer step, infidelity: 0.04413\n",
      "minimizer step, infidelity: 0.04413\n",
      "minimizer step, infidelity: 0.04413\n",
      "minimizer step, infidelity: 0.04413\n",
      "minimizer step, infidelity: 0.04413\n",
      "minimizer step, infidelity: 0.04413\n",
      "minimizer step, infidelity: 0.04413\n",
      "minimizer step, infidelity: 0.04413\n",
      "minimizer step, infidelity: 0.04412\n",
      "minimizer step, infidelity: 0.04412\n",
      "minimizer step, infidelity: 0.04412\n",
      "minimizer step, infidelity: 0.04411\n",
      "minimizer step, infidelity: 0.04411\n",
      "minimizer step, infidelity: 0.04411\n",
      "minimizer step, infidelity: 0.04409\n",
      "minimizer step, infidelity: 0.04409\n",
      "minimizer step, infidelity: 0.04408\n",
      "minimizer step, infidelity: 0.04408\n",
      "minimizer step, infidelity: 0.04407\n",
      "minimizer step, infidelity: 0.04406\n",
      "minimizer step, infidelity: 0.04406\n",
      "minimizer step, infidelity: 0.04406\n",
      "minimizer step, infidelity: 0.04406\n",
      "minimizer step, infidelity: 0.04405\n",
      "minimizer step, infidelity: 0.04405\n",
      "minimizer step, infidelity: 0.04405\n",
      "minimizer step, infidelity: 0.04404\n",
      "minimizer step, infidelity: 0.04404\n",
      "minimizer step, infidelity: 0.04404\n",
      "minimizer step, infidelity: 0.04404\n",
      "minimizer step, infidelity: 0.04404\n",
      "minimizer step, infidelity: 0.04404\n",
      "minimizer step, infidelity: 0.04404\n",
      "minimizer step, infidelity: 0.04404\n",
      "minimizer step, infidelity: 0.04404\n",
      "minimizer step, infidelity: 0.04404\n",
      "optimizer step, infidelity: 0.04404, took 6.76 seconds\n"
     ]
    }
   ],
   "source": [
    "res_goat = optimize_pulses(\n",
    "    objectives = [Objective(initial, H, target)],\n",
    "    pulse_options={\n",
    "        \"p\": {\n",
    "            \"guess\":  p_init, # p0 * sin(p1 * t + p2)\n",
    "            \"bounds\": [(-1, 1), (0, 1), (0, 2*np.pi)],\n",
    "        },\n",
    "        \"q\": {\n",
    "            \"guess\":  q_init, # q0 * sin(q1 * t + q2)\n",
    "            \"bounds\": [(-1, 1), (0, 1), (0, 2*np.pi)],\n",
    "        },\n",
    "        \"r\": {\n",
    "            \"guess\":  r_init, # r0 * sin(r1 * t + r2)\n",
    "            \"bounds\": [(-1, 1), (0, 1), (0, 2*np.pi)],\n",
    "        }\n",
    "    },\n",
    "    time_interval = interval,\n",
    "    algorithm_kwargs = {\n",
    "        \"alg\": \"GOAT\",\n",
    "        \"fid_err_targ\": 0.01,\n",
    "        \"method\": \"basinhopping\",\n",
    "        \"disp\": True,\n",
    "        \"max_iter\": 0, # global optimizer steps\n",
    "        \"seed\": 1,\n",
    "    },\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Control Optimization Result\n",
       "--------------------------\n",
       "- Started at 2023-11-10 17:27:26\n",
       "- Number of objectives: 1\n",
       "- Final fidelity error: 0.04403656960277009\n",
       "- Final parameters: [[1.0, 0.6462771162438175, 2.1425609700529074], [0.9996894160117288, 0.990898249375864, 0.004499661412899851], [0.8444724950457054, 2.379880939421669e-09, 0.2993300539999815]]\n",
       "- Number of iterations: 1\n",
       "- Reason for termination: ['requested number of basinhopping iterations completed successfully']\n",
       "- Ended at 2023-11-10 17:27:33 (6.7618s)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_goat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin_jax(t, α):\n",
    "    return α[0] * jnp.sin(α[1] * t + α[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def sin_x_jax(t, p, **kwargs):\n",
    "    return sin_jax(t, p)\n",
    "\n",
    "@jax.jit\n",
    "def sin_y_jax(t, q, **kwargs):\n",
    "    return sin_jax(t, q)\n",
    "\n",
    "@jax.jit\n",
    "def sin_z_jax(t, r, **kwargs):\n",
    "    return sin_jax(t, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_jax = [ H_d,\n",
    "         [H_c[0], sin_x_jax],\n",
    "         [H_c[1], sin_y_jax],\n",
    "         [H_c[2], sin_z_jax]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimizer step, infidelity: 0.27609\n",
      "minimizer step, infidelity: 0.26607\n",
      "minimizer step, infidelity: 0.24655\n",
      "minimizer step, infidelity: 0.19537\n",
      "minimizer step, infidelity: 0.11597\n",
      "minimizer step, infidelity: 0.09035\n",
      "minimizer step, infidelity: 0.07066\n",
      "minimizer step, infidelity: 0.06813\n",
      "minimizer step, infidelity: 0.06528\n",
      "minimizer step, infidelity: 0.06450\n",
      "minimizer step, infidelity: 0.06266\n",
      "minimizer step, infidelity: 0.05988\n",
      "minimizer step, infidelity: 0.05548\n",
      "minimizer step, infidelity: 0.05310\n",
      "minimizer step, infidelity: 0.05288\n",
      "minimizer step, infidelity: 0.05284\n",
      "minimizer step, infidelity: 0.05275\n",
      "minimizer step, infidelity: 0.05263\n",
      "minimizer step, infidelity: 0.05247\n",
      "minimizer step, infidelity: 0.05233\n",
      "minimizer step, infidelity: 0.05185\n",
      "minimizer step, infidelity: 0.05124\n",
      "minimizer step, infidelity: 0.05073\n",
      "minimizer step, infidelity: 0.05028\n",
      "minimizer step, infidelity: 0.05026\n",
      "minimizer step, infidelity: 0.05021\n",
      "minimizer step, infidelity: 0.05017\n",
      "minimizer step, infidelity: 0.05011\n",
      "minimizer step, infidelity: 0.05005\n",
      "minimizer step, infidelity: 0.05002\n",
      "minimizer step, infidelity: 0.04998\n",
      "minimizer step, infidelity: 0.04997\n",
      "minimizer step, infidelity: 0.04996\n",
      "minimizer step, infidelity: 0.04995\n",
      "minimizer step, infidelity: 0.04993\n",
      "minimizer step, infidelity: 0.04990\n",
      "minimizer step, infidelity: 0.04987\n",
      "minimizer step, infidelity: 0.04979\n",
      "minimizer step, infidelity: 0.04968\n",
      "minimizer step, infidelity: 0.04965\n",
      "minimizer step, infidelity: 0.04964\n",
      "minimizer step, infidelity: 0.04964\n",
      "minimizer step, infidelity: 0.04963\n",
      "minimizer step, infidelity: 0.04961\n",
      "minimizer step, infidelity: 0.04960\n",
      "minimizer step, infidelity: 0.04955\n",
      "minimizer step, infidelity: 0.04942\n",
      "minimizer step, infidelity: 0.04925\n",
      "minimizer step, infidelity: 0.04916\n",
      "minimizer step, infidelity: 0.04873\n",
      "minimizer step, infidelity: 0.04852\n",
      "minimizer step, infidelity: 0.04808\n",
      "minimizer step, infidelity: 0.04802\n",
      "minimizer step, infidelity: 0.04796\n",
      "minimizer step, infidelity: 0.04791\n",
      "minimizer step, infidelity: 0.04785\n",
      "minimizer step, infidelity: 0.04778\n",
      "minimizer step, infidelity: 0.04775\n",
      "minimizer step, infidelity: 0.04774\n",
      "minimizer step, infidelity: 0.04769\n",
      "minimizer step, infidelity: 0.04764\n",
      "minimizer step, infidelity: 0.04754\n",
      "minimizer step, infidelity: 0.04742\n",
      "minimizer step, infidelity: 0.04716\n",
      "minimizer step, infidelity: 0.04668\n",
      "minimizer step, infidelity: 0.04661\n",
      "minimizer step, infidelity: 0.04659\n",
      "minimizer step, infidelity: 0.04646\n",
      "minimizer step, infidelity: 0.04619\n",
      "minimizer step, infidelity: 0.04591\n",
      "minimizer step, infidelity: 0.04589\n",
      "minimizer step, infidelity: 0.04589\n",
      "minimizer step, infidelity: 0.04589\n",
      "minimizer step, infidelity: 0.04575\n",
      "minimizer step, infidelity: 0.04574\n",
      "minimizer step, infidelity: 0.04574\n",
      "minimizer step, infidelity: 0.04573\n",
      "minimizer step, infidelity: 0.04573\n",
      "minimizer step, infidelity: 0.04573\n",
      "minimizer step, infidelity: 0.04573\n",
      "minimizer step, infidelity: 0.04573\n",
      "minimizer step, infidelity: 0.04573\n",
      "minimizer step, infidelity: 0.04572\n",
      "minimizer step, infidelity: 0.04571\n",
      "minimizer step, infidelity: 0.04569\n",
      "minimizer step, infidelity: 0.04565\n",
      "minimizer step, infidelity: 0.04565\n",
      "minimizer step, infidelity: 0.04562\n",
      "minimizer step, infidelity: 0.04560\n",
      "minimizer step, infidelity: 0.04560\n",
      "minimizer step, infidelity: 0.04559\n",
      "minimizer step, infidelity: 0.04558\n",
      "minimizer step, infidelity: 0.04557\n",
      "minimizer step, infidelity: 0.04557\n",
      "minimizer step, infidelity: 0.04557\n",
      "minimizer step, infidelity: 0.04557\n",
      "minimizer step, infidelity: 0.04557\n",
      "minimizer step, infidelity: 0.04557\n",
      "minimizer step, infidelity: 0.04557\n",
      "minimizer step, infidelity: 0.04557\n",
      "minimizer step, infidelity: 0.04557\n",
      "minimizer step, infidelity: 0.04557\n",
      "minimizer step, infidelity: 0.04556\n",
      "minimizer step, infidelity: 0.04555\n",
      "minimizer step, infidelity: 0.04553\n",
      "minimizer step, infidelity: 0.04552\n",
      "minimizer step, infidelity: 0.04548\n",
      "minimizer step, infidelity: 0.04545\n",
      "minimizer step, infidelity: 0.04539\n",
      "minimizer step, infidelity: 0.04534\n",
      "minimizer step, infidelity: 0.04531\n",
      "minimizer step, infidelity: 0.04527\n",
      "minimizer step, infidelity: 0.04525\n",
      "minimizer step, infidelity: 0.04524\n",
      "minimizer step, infidelity: 0.04523\n",
      "minimizer step, infidelity: 0.04522\n",
      "minimizer step, infidelity: 0.04520\n",
      "minimizer step, infidelity: 0.04517\n",
      "minimizer step, infidelity: 0.04514\n",
      "minimizer step, infidelity: 0.04511\n",
      "minimizer step, infidelity: 0.04509\n",
      "minimizer step, infidelity: 0.04508\n",
      "minimizer step, infidelity: 0.04507\n",
      "minimizer step, infidelity: 0.04505\n",
      "minimizer step, infidelity: 0.04503\n",
      "minimizer step, infidelity: 0.04501\n",
      "minimizer step, infidelity: 0.04499\n",
      "minimizer step, infidelity: 0.04494\n",
      "minimizer step, infidelity: 0.04489\n",
      "minimizer step, infidelity: 0.04482\n",
      "minimizer step, infidelity: 0.04480\n",
      "minimizer step, infidelity: 0.04476\n",
      "minimizer step, infidelity: 0.04472\n",
      "minimizer step, infidelity: 0.04469\n",
      "minimizer step, infidelity: 0.04465\n",
      "minimizer step, infidelity: 0.04463\n",
      "minimizer step, infidelity: 0.04460\n",
      "minimizer step, infidelity: 0.04459\n",
      "minimizer step, infidelity: 0.04458\n",
      "minimizer step, infidelity: 0.04457\n",
      "minimizer step, infidelity: 0.04457\n",
      "minimizer step, infidelity: 0.04457\n",
      "minimizer step, infidelity: 0.04457\n",
      "minimizer step, infidelity: 0.04457\n",
      "minimizer step, infidelity: 0.04456\n",
      "minimizer step, infidelity: 0.04455\n",
      "minimizer step, infidelity: 0.04455\n",
      "minimizer step, infidelity: 0.04455\n",
      "minimizer step, infidelity: 0.04455\n",
      "optimizer step, infidelity: 0.04455, took 30.84 seconds\n"
     ]
    }
   ],
   "source": [
    "res_joat = optimize_pulses(\n",
    "    objectives = [Objective(initial, H_jax, target)],\n",
    "    pulse_options={\n",
    "        \"p\": {\n",
    "            \"guess\":  p_init,\n",
    "            \"bounds\": [(-1, 1), (0, 1), (0, 2*np.pi)],\n",
    "        },\n",
    "        \"q\": {\n",
    "            \"guess\":  q_init,\n",
    "            \"bounds\": [(-1, 1), (0, 1), (0, 2*np.pi)],\n",
    "        },\n",
    "        \"r\": {\n",
    "            \"guess\":  r_init,\n",
    "            \"bounds\": [(-1, 1), (0, 1), (0, 2*np.pi)],\n",
    "        }\n",
    "    },\n",
    "    time_interval = interval,\n",
    "    algorithm_kwargs = {\n",
    "        \"alg\": \"JOAT\",\n",
    "        \"fid_err_targ\": 0.01,\n",
    "        \"method\": \"basinhopping\",\n",
    "        \"disp\": True,\n",
    "        \"max_iter\": 0,\n",
    "        \"seed\": 1,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Control Optimization Result\n",
       "--------------------------\n",
       "- Started at 2023-11-10 17:27:33\n",
       "- Number of objectives: 1\n",
       "- Final fidelity error: 0.04455113743096623\n",
       "- Final parameters: [[0.999553068793097, 0.9677108939313139, 1.3896276385936615], [0.7368374722530612, 0.7802926694405331, 1.5571836032831659], [0.3879646737896883, 0.0, 1.7601127919800026]]\n",
       "- Number of iterations: 1\n",
       "- Reason for termination: ['requested number of basinhopping iterations completed successfully']\n",
       "- Ended at 2023-11-10 17:28:04 (30.8379s)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_joat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default integrator settings - local search only - variable time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GOAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimizer step, infidelity: 0.27596\n",
      "minimizer step, infidelity: 0.26577\n",
      "minimizer step, infidelity: 0.24572\n",
      "minimizer step, infidelity: 0.19266\n",
      "minimizer step, infidelity: 0.11669\n",
      "minimizer step, infidelity: 0.10373\n",
      "minimizer step, infidelity: 0.08367\n",
      "minimizer step, infidelity: 0.07925\n",
      "minimizer step, infidelity: 0.06900\n",
      "minimizer step, infidelity: 0.06096\n",
      "minimizer step, infidelity: 0.05621\n",
      "minimizer step, infidelity: 0.05442\n",
      "minimizer step, infidelity: 0.05415\n",
      "minimizer step, infidelity: 0.05395\n",
      "minimizer step, infidelity: 0.05351\n",
      "minimizer step, infidelity: 0.05318\n",
      "minimizer step, infidelity: 0.05249\n",
      "minimizer step, infidelity: 0.05209\n",
      "minimizer step, infidelity: 0.05184\n",
      "minimizer step, infidelity: 0.05163\n",
      "minimizer step, infidelity: 0.05153\n",
      "minimizer step, infidelity: 0.05145\n",
      "minimizer step, infidelity: 0.05140\n",
      "minimizer step, infidelity: 0.05137\n",
      "minimizer step, infidelity: 0.05134\n",
      "minimizer step, infidelity: 0.05133\n",
      "minimizer step, infidelity: 0.05132\n",
      "minimizer step, infidelity: 0.05131\n",
      "minimizer step, infidelity: 0.05127\n",
      "minimizer step, infidelity: 0.05118\n",
      "minimizer step, infidelity: 0.05104\n",
      "minimizer step, infidelity: 0.05090\n",
      "minimizer step, infidelity: 0.05080\n",
      "minimizer step, infidelity: 0.05065\n",
      "minimizer step, infidelity: 0.05050\n",
      "minimizer step, infidelity: 0.05012\n",
      "minimizer step, infidelity: 0.04965\n",
      "minimizer step, infidelity: 0.04914\n",
      "minimizer step, infidelity: 0.04839\n",
      "minimizer step, infidelity: 0.04778\n",
      "minimizer step, infidelity: 0.04717\n",
      "minimizer step, infidelity: 0.04694\n",
      "minimizer step, infidelity: 0.04588\n",
      "minimizer step, infidelity: 0.04558\n",
      "minimizer step, infidelity: 0.04524\n",
      "minimizer step, infidelity: 0.04480\n",
      "minimizer step, infidelity: 0.04437\n",
      "minimizer step, infidelity: 0.04383\n",
      "minimizer step, infidelity: 0.04382\n",
      "minimizer step, infidelity: 0.04364\n",
      "minimizer step, infidelity: 0.04356\n",
      "minimizer step, infidelity: 0.04355\n",
      "minimizer step, infidelity: 0.04352\n",
      "minimizer step, infidelity: 0.04351\n",
      "minimizer step, infidelity: 0.04348\n",
      "minimizer step, infidelity: 0.04344\n",
      "minimizer step, infidelity: 0.04340\n",
      "minimizer step, infidelity: 0.04338\n",
      "minimizer step, infidelity: 0.04337\n",
      "minimizer step, infidelity: 0.04336\n",
      "minimizer step, infidelity: 0.04334\n",
      "minimizer step, infidelity: 0.04327\n",
      "minimizer step, infidelity: 0.04324\n",
      "minimizer step, infidelity: 0.04312\n",
      "minimizer step, infidelity: 0.04303\n",
      "minimizer step, infidelity: 0.04278\n",
      "minimizer step, infidelity: 0.04251\n",
      "minimizer step, infidelity: 0.04219\n",
      "minimizer step, infidelity: 0.04183\n",
      "minimizer step, infidelity: 0.04107\n",
      "minimizer step, infidelity: 0.04080\n",
      "minimizer step, infidelity: 0.04033\n",
      "minimizer step, infidelity: 0.03976\n",
      "minimizer step, infidelity: 0.03950\n",
      "minimizer step, infidelity: 0.03929\n",
      "minimizer step, infidelity: 0.03883\n",
      "minimizer step, infidelity: 0.03842\n",
      "minimizer step, infidelity: 0.03813\n",
      "minimizer step, infidelity: 0.03802\n",
      "minimizer step, infidelity: 0.03797\n",
      "minimizer step, infidelity: 0.03785\n",
      "minimizer step, infidelity: 0.03779\n",
      "minimizer step, infidelity: 0.03776\n",
      "minimizer step, infidelity: 0.03771\n",
      "minimizer step, infidelity: 0.03765\n",
      "minimizer step, infidelity: 0.03760\n",
      "minimizer step, infidelity: 0.03753\n",
      "minimizer step, infidelity: 0.03735\n",
      "minimizer step, infidelity: 0.03684\n",
      "minimizer step, infidelity: 0.03643\n",
      "minimizer step, infidelity: 0.03588\n",
      "minimizer step, infidelity: 0.03537\n",
      "minimizer step, infidelity: 0.03366\n",
      "minimizer step, infidelity: 0.03130\n",
      "minimizer step, infidelity: 0.03051\n",
      "minimizer step, infidelity: 0.02937\n",
      "minimizer step, infidelity: 0.02771\n",
      "minimizer step, infidelity: 0.02608\n",
      "minimizer step, infidelity: 0.02516\n",
      "minimizer step, infidelity: 0.02493\n",
      "minimizer step, infidelity: 0.02492\n",
      "minimizer step, infidelity: 0.02492\n",
      "minimizer step, infidelity: 0.02492\n",
      "optimizer step, infidelity: 0.02492, took 23.16 seconds\n"
     ]
    }
   ],
   "source": [
    "res_joat = optimize_pulses(\n",
    "    objectives = [Objective(initial, H_jax, target)],\n",
    "    pulse_options={\n",
    "        \"p\": {\n",
    "            \"guess\":  p_init,\n",
    "            \"bounds\": [(-1, 1), (0, 1), (0, 2*np.pi)],\n",
    "        },\n",
    "        \"q\": {\n",
    "            \"guess\":  q_init,\n",
    "            \"bounds\": [(-1, 1), (0, 1), (0, 2*np.pi)],\n",
    "        },\n",
    "        \"r\": {\n",
    "            \"guess\":  r_init,\n",
    "            \"bounds\": [(-1, 1), (0, 1), (0, 2*np.pi)],\n",
    "        }\n",
    "    },\n",
    "    time_interval = interval,\n",
    "    time_options = {\n",
    "        \"guess\": interval.evo_time,\n",
    "        \"bounds\": (0, 2*interval.evo_time),\n",
    "    },\n",
    "    algorithm_kwargs = {\n",
    "        \"alg\": \"JOAT\",\n",
    "        \"fid_err_targ\": 0.01,\n",
    "        \"method\": \"basinhopping\",\n",
    "        \"disp\": True,\n",
    "        \"max_iter\": 0,\n",
    "        \"seed\": 1,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Control Optimization Result\n",
       "--------------------------\n",
       "- Started at 2023-11-10 17:27:26\n",
       "- Number of objectives: 1\n",
       "- Final fidelity error: 0.04403656960277009\n",
       "- Final parameters: [[1.0, 0.6462771162438175, 2.1425609700529074], [0.9996894160117288, 0.990898249375864, 0.004499661412899851], [0.8444724950457054, 2.379880939421669e-09, 0.2993300539999815]]\n",
       "- Number of iterations: 1\n",
       "- Reason for termination: ['requested number of basinhopping iterations completed successfully']\n",
       "- Ended at 2023-11-10 17:27:33 (6.7618s)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_goat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimizer step, infidelity: 0.27596\n",
      "minimizer step, infidelity: 0.26577\n",
      "minimizer step, infidelity: 0.24572\n",
      "minimizer step, infidelity: 0.19266\n",
      "minimizer step, infidelity: 0.11669\n",
      "minimizer step, infidelity: 0.10373\n",
      "minimizer step, infidelity: 0.08367\n",
      "minimizer step, infidelity: 0.07925\n",
      "minimizer step, infidelity: 0.06900\n",
      "minimizer step, infidelity: 0.06096\n",
      "minimizer step, infidelity: 0.05621\n",
      "minimizer step, infidelity: 0.05442\n",
      "minimizer step, infidelity: 0.05415\n",
      "minimizer step, infidelity: 0.05395\n",
      "minimizer step, infidelity: 0.05351\n",
      "minimizer step, infidelity: 0.05318\n",
      "minimizer step, infidelity: 0.05249\n",
      "minimizer step, infidelity: 0.05209\n",
      "minimizer step, infidelity: 0.05184\n",
      "minimizer step, infidelity: 0.05163\n",
      "minimizer step, infidelity: 0.05153\n",
      "minimizer step, infidelity: 0.05145\n",
      "minimizer step, infidelity: 0.05140\n",
      "minimizer step, infidelity: 0.05137\n",
      "minimizer step, infidelity: 0.05134\n",
      "minimizer step, infidelity: 0.05133\n",
      "minimizer step, infidelity: 0.05132\n",
      "minimizer step, infidelity: 0.05131\n",
      "minimizer step, infidelity: 0.05127\n",
      "minimizer step, infidelity: 0.05118\n",
      "minimizer step, infidelity: 0.05104\n",
      "minimizer step, infidelity: 0.05090\n",
      "minimizer step, infidelity: 0.05080\n",
      "minimizer step, infidelity: 0.05065\n",
      "minimizer step, infidelity: 0.05050\n",
      "minimizer step, infidelity: 0.05012\n",
      "minimizer step, infidelity: 0.04965\n",
      "minimizer step, infidelity: 0.04914\n",
      "minimizer step, infidelity: 0.04839\n",
      "minimizer step, infidelity: 0.04778\n",
      "minimizer step, infidelity: 0.04717\n",
      "minimizer step, infidelity: 0.04694\n",
      "minimizer step, infidelity: 0.04588\n",
      "minimizer step, infidelity: 0.04558\n",
      "minimizer step, infidelity: 0.04524\n",
      "minimizer step, infidelity: 0.04480\n",
      "minimizer step, infidelity: 0.04437\n",
      "minimizer step, infidelity: 0.04383\n",
      "minimizer step, infidelity: 0.04382\n",
      "minimizer step, infidelity: 0.04364\n",
      "minimizer step, infidelity: 0.04356\n",
      "minimizer step, infidelity: 0.04355\n",
      "minimizer step, infidelity: 0.04352\n",
      "minimizer step, infidelity: 0.04351\n",
      "minimizer step, infidelity: 0.04348\n",
      "minimizer step, infidelity: 0.04344\n",
      "minimizer step, infidelity: 0.04340\n",
      "minimizer step, infidelity: 0.04338\n",
      "minimizer step, infidelity: 0.04337\n",
      "minimizer step, infidelity: 0.04336\n",
      "minimizer step, infidelity: 0.04334\n",
      "minimizer step, infidelity: 0.04327\n",
      "minimizer step, infidelity: 0.04324\n",
      "minimizer step, infidelity: 0.04312\n",
      "minimizer step, infidelity: 0.04303\n",
      "minimizer step, infidelity: 0.04278\n",
      "minimizer step, infidelity: 0.04251\n",
      "minimizer step, infidelity: 0.04219\n",
      "minimizer step, infidelity: 0.04183\n",
      "minimizer step, infidelity: 0.04107\n",
      "minimizer step, infidelity: 0.04080\n",
      "minimizer step, infidelity: 0.04033\n",
      "minimizer step, infidelity: 0.03976\n",
      "minimizer step, infidelity: 0.03950\n",
      "minimizer step, infidelity: 0.03929\n",
      "minimizer step, infidelity: 0.03883\n",
      "minimizer step, infidelity: 0.03842\n",
      "minimizer step, infidelity: 0.03813\n",
      "minimizer step, infidelity: 0.03802\n",
      "minimizer step, infidelity: 0.03797\n",
      "minimizer step, infidelity: 0.03785\n",
      "minimizer step, infidelity: 0.03779\n",
      "minimizer step, infidelity: 0.03776\n",
      "minimizer step, infidelity: 0.03771\n",
      "minimizer step, infidelity: 0.03765\n",
      "minimizer step, infidelity: 0.03760\n",
      "minimizer step, infidelity: 0.03753\n",
      "minimizer step, infidelity: 0.03735\n",
      "minimizer step, infidelity: 0.03684\n",
      "minimizer step, infidelity: 0.03643\n",
      "minimizer step, infidelity: 0.03588\n",
      "minimizer step, infidelity: 0.03537\n",
      "minimizer step, infidelity: 0.03366\n",
      "minimizer step, infidelity: 0.03130\n",
      "minimizer step, infidelity: 0.03051\n",
      "minimizer step, infidelity: 0.02937\n",
      "minimizer step, infidelity: 0.02771\n",
      "minimizer step, infidelity: 0.02608\n",
      "minimizer step, infidelity: 0.02516\n",
      "minimizer step, infidelity: 0.02493\n",
      "minimizer step, infidelity: 0.02492\n",
      "minimizer step, infidelity: 0.02492\n",
      "minimizer step, infidelity: 0.02492\n",
      "optimizer step, infidelity: 0.02492, took 23.11 seconds\n"
     ]
    }
   ],
   "source": [
    "res_joat = optimize_pulses(\n",
    "    objectives = [Objective(initial, H_jax, target)],\n",
    "    pulse_options={\n",
    "        \"p\": {\n",
    "            \"guess\":  p_init,\n",
    "            \"bounds\": [(-1, 1), (0, 1), (0, 2*np.pi)],\n",
    "        },\n",
    "        \"q\": {\n",
    "            \"guess\":  q_init,\n",
    "            \"bounds\": [(-1, 1), (0, 1), (0, 2*np.pi)],\n",
    "        },\n",
    "        \"r\": {\n",
    "            \"guess\":  r_init,\n",
    "            \"bounds\": [(-1, 1), (0, 1), (0, 2*np.pi)],\n",
    "        }\n",
    "    },\n",
    "    time_interval = interval,\n",
    "    time_options = {\n",
    "        \"guess\": interval.evo_time,\n",
    "        \"bounds\": (0, 2*interval.evo_time),\n",
    "    },\n",
    "    algorithm_kwargs = {\n",
    "        \"alg\": \"JOAT\",\n",
    "        \"fid_err_targ\": 0.01,\n",
    "        \"method\": \"basinhopping\",\n",
    "        \"disp\": True,\n",
    "        \"max_iter\": 0,\n",
    "        \"seed\": 1,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Control Optimization Result\n",
       "--------------------------\n",
       "- Started at 2023-11-10 17:28:28\n",
       "- Number of objectives: 1\n",
       "- Final fidelity error: 0.02492020332425142\n",
       "- Final parameters: [[1.0, 0.9485234732252087, 2.0175236634621174], [0.9999985248371083, 0.9174976046839698, 0.9463379704948467], [0.9605607074573544, 1.0, 0.9800389764496323], [3.846593105352696]]\n",
       "- Number of iterations: 1\n",
       "- Reason for termination: ['requested number of basinhopping iterations completed successfully']\n",
       "- Ended at 2023-11-10 17:28:51 (23.11s)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_joat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same integrator settings - global search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GOAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer step, infidelity: 0.14197, took 0.08 seconds\n",
      "minimizer step, infidelity: 0.04040\n",
      "minimizer step, infidelity: 0.03003\n",
      "minimizer step, infidelity: 0.02714\n",
      "minimizer step, infidelity: 0.02697\n",
      "optimizer step, infidelity: 0.02697, took 0.95 seconds\n",
      "optimizer step, infidelity: 0.01949, took 11.79 seconds\n",
      "optimizer step, infidelity: 0.01761, took 0.00 seconds\n",
      "minimizer step, infidelity: 0.01382\n",
      "minimizer step, infidelity: 0.01345\n",
      "minimizer step, infidelity: 0.01334\n",
      "minimizer step, infidelity: 0.01329\n",
      "optimizer step, infidelity: 0.01329, took 0.53 seconds\n"
     ]
    }
   ],
   "source": [
    "res_goat = optimize_pulses(\n",
    "    objectives = [Objective(initial, H, target)],\n",
    "    pulse_options={\n",
    "        \"p\": {\n",
    "            \"guess\":  p_init, # p0 * sin(p1 * t + p2)\n",
    "            \"bounds\": [(-1, 1), (0, 1), (0, 2*np.pi)],\n",
    "        },\n",
    "        \"q\": {\n",
    "            \"guess\":  q_init, # q0 * sin(q1 * t + q2)\n",
    "            \"bounds\": [(-1, 1), (0, 1), (0, 2*np.pi)],\n",
    "        },\n",
    "        \"r\": {\n",
    "            \"guess\":  r_init, # r0 * sin(r1 * t + r2)\n",
    "            \"bounds\": [(-1, 1), (0, 1), (0, 2*np.pi)],\n",
    "        }\n",
    "    },\n",
    "    time_interval = interval,\n",
    "    time_options = {\n",
    "        \"guess\": interval.evo_time,\n",
    "        \"bounds\": (0, 2*interval.evo_time),\n",
    "    },\n",
    "    algorithm_kwargs = {\n",
    "        \"alg\": \"GOAT\",\n",
    "        \"method\": \"dual_annealing\",\n",
    "        \"fid_err_targ\": 0.01,\n",
    "        \"disp\": True,\n",
    "        \"max_iter\": 100,\n",
    "        \"seed\": 1,\n",
    "    },\n",
    "    integrator_kwargs = {\n",
    "        \"atol\": 1e-6,\n",
    "        \"rtol\": 1e-6,\n",
    "        \"method\": \"dop853\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Control Optimization Result\n",
       "--------------------------\n",
       "- Started at 2023-11-10 17:30:34\n",
       "- Number of objectives: 1\n",
       "- Final fidelity error: 0.01328501973909172\n",
       "- Final parameters: [[0.16275264534575196, 0.14666474720376024, 4.548448377528713], [0.3352717573102745, 0.726879603957995, 4.0035211375470405], [0.9583942759452505, 0.7371399968016987, 5.8964259935535885], [2.4718503482401557]]\n",
       "- Number of iterations: 100\n",
       "- Reason for termination: ['Maximum number of iteration reached']\n",
       "- Ended at 2023-11-10 17:31:05 (30.7651s)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_goat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer step, infidelity: 0.13791, took 1.75 seconds\n",
      "minimizer step, infidelity: 0.11816\n",
      "minimizer step, infidelity: 0.08924\n",
      "minimizer step, infidelity: 0.08275\n",
      "minimizer step, infidelity: 0.08171\n",
      "minimizer step, infidelity: 0.08100\n",
      "minimizer step, infidelity: 0.08044\n",
      "minimizer step, infidelity: 0.07963\n",
      "minimizer step, infidelity: 0.07847\n",
      "minimizer step, infidelity: 0.07733\n",
      "minimizer step, infidelity: 0.07577\n",
      "minimizer step, infidelity: 0.07294\n",
      "minimizer step, infidelity: 0.06826\n",
      "minimizer step, infidelity: 0.06606\n",
      "minimizer step, infidelity: 0.06275\n",
      "minimizer step, infidelity: 0.05744\n",
      "minimizer step, infidelity: 0.04918\n",
      "minimizer step, infidelity: 0.04540\n",
      "minimizer step, infidelity: 0.03948\n",
      "minimizer step, infidelity: 0.03308\n",
      "minimizer step, infidelity: 0.03166\n",
      "minimizer step, infidelity: 0.02954\n",
      "minimizer step, infidelity: 0.02701\n",
      "minimizer step, infidelity: 0.02656\n",
      "minimizer step, infidelity: 0.02583\n",
      "minimizer step, infidelity: 0.02488\n",
      "minimizer step, infidelity: 0.02422\n",
      "minimizer step, infidelity: 0.02315\n",
      "minimizer step, infidelity: 0.02149\n",
      "minimizer step, infidelity: 0.02041\n",
      "minimizer step, infidelity: 0.01855\n",
      "minimizer step, infidelity: 0.01609\n",
      "minimizer step, infidelity: 0.01528\n",
      "minimizer step, infidelity: 0.01441\n",
      "minimizer step, infidelity: 0.01355\n",
      "minimizer step, infidelity: 0.01271\n",
      "minimizer step, infidelity: 0.01218\n",
      "minimizer step, infidelity: 0.01120\n",
      "minimizer step, infidelity: 0.01077\n",
      "minimizer step, infidelity: 0.01035\n",
      "minimizer step, infidelity: 0.00978\n",
      "fid_err_targ reached, terminating minimization\n",
      "optimizer step, infidelity: 0.05751, took 15.06 seconds\n",
      "minimizer step, infidelity: 0.04972\n",
      "minimizer step, infidelity: 0.03955\n",
      "minimizer step, infidelity: 0.03766\n",
      "minimizer step, infidelity: 0.03702\n",
      "minimizer step, infidelity: 0.03680\n",
      "minimizer step, infidelity: 0.03476\n",
      "minimizer step, infidelity: 0.03055\n",
      "minimizer step, infidelity: 0.02941\n",
      "minimizer step, infidelity: 0.02777\n",
      "minimizer step, infidelity: 0.02585\n",
      "minimizer step, infidelity: 0.02417\n",
      "minimizer step, infidelity: 0.02224\n",
      "minimizer step, infidelity: 0.02014\n",
      "minimizer step, infidelity: 0.01820\n",
      "minimizer step, infidelity: 0.01637\n",
      "minimizer step, infidelity: 0.01539\n",
      "minimizer step, infidelity: 0.01405\n",
      "minimizer step, infidelity: 0.01300\n",
      "minimizer step, infidelity: 0.01150\n",
      "minimizer step, infidelity: 0.01119\n",
      "minimizer step, infidelity: 0.01068\n",
      "minimizer step, infidelity: 0.00988\n",
      "fid_err_targ reached, terminating minimization\n",
      "optimizer step, infidelity: 0.04757, took 15.87 seconds\n",
      "optimizer step, infidelity: 0.04572, took 0.10 seconds\n",
      "optimizer step, infidelity: 0.04438, took 0.02 seconds\n",
      "minimizer step, infidelity: 0.04161\n",
      "minimizer step, infidelity: 0.04115\n",
      "minimizer step, infidelity: 0.04047\n",
      "minimizer step, infidelity: 0.03994\n",
      "minimizer step, infidelity: 0.03949\n",
      "minimizer step, infidelity: 0.03865\n",
      "minimizer step, infidelity: 0.03202\n",
      "minimizer step, infidelity: 0.03009\n",
      "minimizer step, infidelity: 0.02763\n",
      "minimizer step, infidelity: 0.02394\n",
      "minimizer step, infidelity: 0.02126\n",
      "minimizer step, infidelity: 0.02022\n",
      "minimizer step, infidelity: 0.01878\n",
      "minimizer step, infidelity: 0.01667\n",
      "minimizer step, infidelity: 0.01539\n",
      "minimizer step, infidelity: 0.01442\n",
      "minimizer step, infidelity: 0.01309\n",
      "minimizer step, infidelity: 0.01233\n",
      "minimizer step, infidelity: 0.01156\n",
      "minimizer step, infidelity: 0.01070\n",
      "minimizer step, infidelity: 0.01002\n",
      "minimizer step, infidelity: 0.00904\n",
      "fid_err_targ reached, terminating minimization\n",
      "optimizer step, infidelity: 0.03043, took 8.94 seconds\n",
      "optimizer step, infidelity: 0.03041, took 0.01 seconds\n",
      "optimizer step, infidelity: 0.03031, took 0.01 seconds\n",
      "minimizer step, infidelity: 0.00461\n",
      "fid_err_targ reached, terminating minimization\n",
      "optimizer step, infidelity: 0.00461, took 0.32 seconds\n",
      "fid_err_targ reached, terminating optimization\n"
     ]
    }
   ],
   "source": [
    "res_joat = optimize_pulses(\n",
    "    objectives = [Objective(initial, H_jax, target)],\n",
    "    pulse_options={\n",
    "        \"p\": {\n",
    "            \"guess\":  p_init,\n",
    "            \"bounds\": [(-1, 1), (0, 1), (0, 2*np.pi)],\n",
    "        },\n",
    "        \"q\": {\n",
    "            \"guess\":  q_init,\n",
    "            \"bounds\": [(-1, 1), (0, 1), (0, 2*np.pi)],\n",
    "        },\n",
    "        \"r\": {\n",
    "            \"guess\":  r_init,\n",
    "            \"bounds\": [(-1, 1), (0, 1), (0, 2*np.pi)],\n",
    "        }\n",
    "    },\n",
    "    time_interval = interval,\n",
    "    time_options = {\n",
    "        \"guess\": interval.evo_time,\n",
    "        \"bounds\": (0, 2*interval.evo_time),\n",
    "    },\n",
    "    algorithm_kwargs = {\n",
    "        \"alg\": \"JOAT\",\n",
    "        \"method\": \"dual_annealing\",\n",
    "        \"fid_err_targ\": 0.01,\n",
    "        \"disp\": True,\n",
    "        \"max_iter\": 100,\n",
    "        \"seed\": 1,\n",
    "    },\n",
    "    integrator_kwargs = {\n",
    "        \"stepsize_controller\": PIDController(\n",
    "            atol = 1e-6,\n",
    "            rtol = 1e-6,\n",
    "        ),\n",
    "        \"solver\": Dopri8(),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Control Optimization Result\n",
       "--------------------------\n",
       "- Started at 2023-11-10 17:31:05\n",
       "- Number of objectives: 1\n",
       "- Final fidelity error: 0.004609634541730921\n",
       "- Final parameters: [[0.4037346765474621, 0.6412245011923677, 1.1238995677340033], [0.014759329784488393, 0.17776074280489226, 1.998389686022864], [0.8417012947771373, 0.33787128389855775, 1.4778811366267417], [1.227293080135112]]\n",
       "- Number of iterations: 71\n",
       "- Reason for termination: fid_err_targ reached\n",
       "- Ended at 2023-11-10 17:31:47 (42.0568s)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_joat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qutip-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
